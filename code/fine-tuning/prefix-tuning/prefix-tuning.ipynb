{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2cc8d1b9-0594-4cd4-955f-8c34f73b1880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ec5385c-84a0-47e9-ae77-bd787129eb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visible: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Visible:\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dfebe0f9-8dee-46d2-adb7-53ad6b059e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: 0\n",
      "Device name: NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "print(\"Using device:\", torch.cuda.current_device())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec1594b5-0fa1-43ea-8579-af872b99733b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from peft import PrefixTuningConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c39befe2-deed-4309-870c-5f541e7db5e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecc276ecb73c40368df772d5318bd9f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "138ac7cc9ade4db5ae3a6539a858f71b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "818caf05e13b434da10f8049b1f36851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and preprocess the dataset\n",
    "raw_dataset = load_dataset(\"omi-health/medical-dialogue-to-soap-summary\", keep_in_memory=True)\n",
    "\n",
    "def preprocess_dialogue(example):\n",
    "    dialogue = example[\"dialogue\"]\n",
    "    soap = example[\"soap\"]\n",
    "    dialogue = re.sub(r'[^A-Za-z0-9\\s\\.,:?-]', '', dialogue).strip()\n",
    "    soap = re.sub(r'[^A-Za-z0-9\\s\\.,:?-]', '', soap).strip()\n",
    "    dialogue = dialogue.replace(\"Doctor:\", \"[Doctor]:\").replace(\"Patient:\", \"[Patient]:\")\n",
    "    return {\"dialogue\": dialogue, \"soap\": soap}\n",
    "\n",
    "processed = raw_dataset.map(preprocess_dialogue, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc341dd5-3044-43e7-9efd-87039501e7cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b6b56661d4c4560a963f572a6d5a07b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3802793d9c04787ba79b22f4547bcf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b870e487b2f4086bd17887742bd2b43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b6f3af38b6b4d86ad7b7a620ab3e769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e21a50991a7d4617977f2f6cccbb5cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96197854666d484891377c2ea1b6b056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/e19431/miniconda3/envs/venv/lib/python3.11/site-packages/transformers/quantizers/auto.py:222: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32c0c4d211474f47bd8b13c9c6bfcca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "981d00323cec47d59584ba79ee4a50c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/230 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load tokenizer and model with 8-bit quantization via BitsAndBytesConfig\n",
    "model_name = \"unsloth/Llama-3.2-3B-bnb-4bit\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, force_download=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"unsloth/Llama-3.2-3B-bnb-4bit\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\":0}\n",
    ")\n",
    "\n",
    "# 3) Configure PEFT prefix tuning (\n",
    "peft_config = PrefixTuningConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    inference_mode=False,\n",
    "    num_virtual_tokens=20,\n",
    "    prefix_projection=True\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4aeba60d-185c-4f6d-a833-77632f5acbea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "014def2e467c4323aab9a9b8e427c8d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb1be6352afc47b8bd6565dbddc28b31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc223a8f2a4a4a64955a604724d43c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenization function for seq2seq\n",
    "\n",
    "max_prompt = 512\n",
    "max_target = 256\n",
    "max_total = max_prompt + max_target + 1\n",
    "\n",
    "def tokenize_fn(example):\n",
    "    prompt = \"Summarize the following medical dialogue into a SOAP note:\\n\" + example[\"dialogue\"]\n",
    "    prompt_ids = tokenizer(prompt, truncation=True, max_length=max_prompt, add_special_tokens=False)[\"input_ids\"]\n",
    "    target_ids = tokenizer(example[\"soap\"], truncation=True, max_length=max_target, add_special_tokens=False)[\"input_ids\"]\n",
    "    seq = prompt_ids + target_ids + [tokenizer.eos_token_id]\n",
    "    if len(seq) > max_total:\n",
    "        seq = seq[-max_total:]\n",
    "    attention_mask = [1] * len(seq)\n",
    "    pad_len = max_total - len(seq)\n",
    "    seq += [tokenizer.pad_token_id] * pad_len\n",
    "    attention_mask += [0] * pad_len\n",
    "    labels = [-100] * len(prompt_ids) + target_ids + [tokenizer.eos_token_id]\n",
    "    labels = labels[-max_total:]\n",
    "    labels += [-100] * pad_len\n",
    "    return {\"input_ids\": seq, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "# Apply tokenization to all splits\n",
    "tokenized = processed.map(tokenize_fn, batched=False, remove_columns=[\"dialogue\",\"soap\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7658ad83-9f90-4d07-bd36-2fa0bbe079dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tokenized[\"train\"]\n",
    "eval_ds  = tokenized[\"validation\"]\n",
    "test_ds  = tokenized[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0550ba39-e091-483e-b031-ee46666726ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "42a2fb51-7ff1-4be1-80b9-810b8db6c45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments (disable load_best_model_at_end for PEFT)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama-7b-prefix-soap\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    fp16=True,\n",
    "    logging_steps=100,\n",
    "    eval_steps=500,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=False,  # disable for prefix tuning\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "19e3d30a-640f-4f24-8011-9fb62484649e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_309755/64889911.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1737' max='1737' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1737/1737 1:56:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.627500</td>\n",
       "      <td>0.611156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.586700</td>\n",
       "      <td>0.583902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.549300</td>\n",
       "      <td>0.575574</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1737, training_loss=0.610755940175427, metrics={'train_runtime': 7016.3525, 'train_samples_per_second': 3.955, 'train_steps_per_second': 0.248, 'total_flos': 3.60908187950592e+17, 'train_loss': 0.610755940175427, 'epoch': 3.0})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Trainer and start training\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "232d6991-4ec9-4219-a219-a257e226b127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ./llama-3b-prefix-soap ✅\n"
     ]
    }
   ],
   "source": [
    "# Save the prefix-tuned model\n",
    "\n",
    "path = \"./llama-3b-prefix-soap\"\n",
    "model.save_pretrained(path)\n",
    "tokenizer.save_pretrained(path)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {path} ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "57a33d05-ba04-4e01-ba91-d7ad07c0138f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dialogue:\n",
      " [Doctor]: Hello, can you please tell me about your past medical history?\n",
      "[Patient]: Hi, I dont have any past medical history.\n",
      "[Doctor]: Okay. What brings you in today?\n",
      "[Patient]: Ive been experiencing painless blurry vision in my right eye for a week now. Ive also had intermittent fevers, headache, body aches, and a nonpruritic maculopapular rash on my lower legs for the past 6 months.\n",
      "[Doctor]: Thank you for sharing that. Have you had any other symptoms such as neck stiffness, nausea, vomiting, Raynauds phenomenon, oral ulcerations, chest pain, shortness of breath, abdominal pain, or photosensitivity?\n",
      "[Patient]: No, only an isolated episode of left knee swelling and testicular swelling in the past.\n",
      "[Doctor]: Do you work with any toxic substances or have any habits like smoking, drinking, or illicit drug use?\n",
      "[Patient]: No, I work as a flooring installer and I dont have any toxic habits.\n",
      "[Doctor]: Alright. We checked your vital signs and they were normal. During the physical exam, we found bilateral papilledema and optic nerve erythema in your right eye, which was greater than in your left eye. You also have a right inferior nasal quadrant visual field defect and a right afferent pupillary defect. Your muscle strength and reflexes were normal, and your sensation to light touch, pinprick, vibration, and proprioception was intact. We also noticed the maculopapular rash on your bilateral lower extremities.\n",
      "[Patient]: Oh, I see.\n",
      "[Doctor]: Your admitting labs showed some abnormal results. You have microcytic anemia with a hemoglobin of 11.6 gmdL, hematocrit of 35.3, and mean corpuscular volume of 76.9 fL. You also have hyponatremia with a sodium level of 133 mmolL. Your erythrocyte sedimentation rate ESR is elevated at 33 mmhr, and your C-reactive protein CRP is also elevated at 13.3 mgL. Your urinalysis did not show any protein or blood.\n",
      "[Patient]: Okay. What does that mean?\n",
      "[Doctor]: These results could indicate an underlying inflammatory or infectious process. We also performed a lumbar puncture, which showed clear and colorless fluid, 2 red blood cells per microliter, and 56 white blood cells per microliter.\n",
      "[Patient]: So, whats the next step?\n",
      "[Doctor]: We need to investigate further to determine the cause of your symptoms. Well run additional tests and consult with a specialist to get a clearer understanding of your condition. In the meantime, well monitor your symptoms and provide supportive care. Well keep you informed about any new findings and discuss the best course of treatment.\n",
      "[Patient]: Alright, thank you, Doctor.\n",
      "Reference SOAP:\n",
      " S: The patient, a flooring installer with no significant past medical history, presents with painless blurry vision in the right eye for one week, intermittent fevers, headaches, body aches, and a nonpruritic maculopapular rash on the lower legs for six months. The patient also reports an isolated episode of left knee and testicular swelling in the past but denies any neck stiffness, nausea, vomiting, Raynauds phenomenon, oral ulcerations, chest pain, shortness of breath, abdominal pain, or photosensitivity. No history of exposure to toxic substances or habits related to smoking, drinking, or illicit drug use.\n",
      "O: Vital signs are normal. Physical examination reveals bilateral papilledema, greater optic nerve erythema in the right eye, a right inferior nasal quadrant visual field defect, and a right afferent pupillary defect. Muscle strength, reflexes, and sensation to light touch, pinprick, vibration, and proprioception are intact. A maculopapular rash is noted on bilateral lower extremities. Lab findings include microcytic anemia Hemoglobin: 11.6 gmdL, Hematocrit: 35.3, MCV: 76.9 fL, hyponatremia Sodium: 133 mmolL, elevated ESR 33 mmhr, and CRP 13.3 mgL. Urinalysis is normal. Lumbar puncture shows clear fluid, 2 RBCsL, and 56 WBCsL.\n",
      "A: The patients symptoms and findings suggest a possible inflammatory or infectious process affecting multiple systems, including the central nervous system, as indicated by papilledema and abnormal lumbar puncture results. Differential diagnoses could include autoimmune conditions, infectious diseases, or other systemic inflammatory disorders. The presence of optic nerve involvement and systemic symptoms necessitates further investigation to narrow down the causes.\n",
      "P: Plan to conduct additional diagnostic tests to explore underlying causes, including imaging studies and specific serological tests. Consultation with a neurologist and possibly a rheumatologist or infectious disease specialist is recommended. Monitor the patients symptoms closely and provide supportive care as needed. Educate the patient about the findings and the importance of follow-up for further evaluation and management. Ensure the patient understands the potential seriousness of the symptoms and the need for thorough investigation and possibly long-term management.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/e19431/miniconda3/envs/venv/lib/python3.11/site-packages/peft/peft_model.py:2060: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.\n",
      "  warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated SOAP:\n",
      " Summarize the following medical dialogue into a SOAP note:\n",
      "[Doctor]: Hello, can you please tell me about your past medical history?\n",
      "[Patient]: Hi, I dont have any past medical history.\n",
      "[Doctor]: Okay. What brings you in today?\n",
      "[Patient]: Ive been experiencing painless blurry vision in my right eye for a week now. Ive also had intermittent fevers, headache, body aches, and a nonpruritic maculopapular rash on my lower legs for the past 6 months.\n",
      "[Doctor]: Thank you for sharing that. Have you had any other symptoms such as neck stiffness, nausea, vomiting, Raynauds phenomenon, oral ulcerations, chest pain, shortness of breath, abdominal pain, or photosensitivity?\n",
      "[Patient]: No, only an isolated episode of left knee swelling and testicular swelling in the past.\n",
      "[Doctor]: Do you work with any toxic substances or have any habits like smoking, drinking, or illicit drug use?\n",
      "[Patient]: No, I work as a flooring installer and I dont have any toxic habits.\n",
      "[Doctor]: Alright. We checked your vital signs and they were normal. During the physical exam, we found bilateral papilledema and optic nerve erythema in your right eye, which was greater than in your left eye. You also have a right inferior nasal quadrant visual field defect and a right afferent pupillary defect. Your muscle strength and reflexes were normal, and your sensation to light touch, pinprick, vibration, and proprioception was intact. We also noticed the maculopapular rash on your bilateral lower extremities.\n",
      "[Patient]: Oh, I see.\n",
      "[Doctor]: Your admitting labs showed some abnormal results. You have microcytic anemia with a hemoglobin of 11.6 gmdL, hematocrit of 35.3, and mean corpuscular volume of 76.9 fL. You also have hyponatremia with a sodium level of 133 mmolL. Your erythrocyte sedimentation rate ESR is elevated at 33 mmhr, and your C-reactive protein CRP is also elevated at 13.3 mgL. Your urinalysis did not show any protein or blood.\n",
      "[Patient]: Okay. What does that mean?\n",
      "[Doctor]: These results could indicate an underlying inflammatory or infectious process. We also performed a lumbar puncture, which showed clear and colorless fluid, 2 red blood cells per microliter, and 56 white blood cells per microliter.\n",
      "[Patient]: So, whats the next step?\n",
      "[Doctor]: We need to investigate further to determine the cause of your symptoms. Well run additional tests and consult with a specialist to get a clearer understanding of your condition. In the meantime, well monitor your symptoms and provide supportive care. Well keep you informed about any new findings and discuss the best course of treatment.\n",
      "[Patient]: Alright, thank you, Doctor.S: The patient, a flooring installer with no known toxic habits, presents with a one-week history of painless blurry vision in the right eye, intermittent fevers, headache, body aches, and a nonpruritic maculopapular rash on the lower legs for the past 6 months. The patient denies any neck stiffness, nausea, vomiting, Raynauds phenomenon, oral ulcerations, chest pain, shortness of breath, abdominal pain, or photosensitivity. Past medical history includes isolated episodes of left knee swelling and testicular swelling.\n",
      "O: Vital signs are normal. Physical exam reveals bilateral papilledema and optic nerve erythema in the right eye, a right inferior nasal quadrant visual field defect, and a right afferent pupillary defect. Muscle strength and reflexes\n"
     ]
    }
   ],
   "source": [
    "# Generation helper and test example\n",
    "\n",
    "def generate_soap(dialogue_text):\n",
    "    prompt = \"Summarize the following medical dialogue into a SOAP note:\\n\" + dialogue_text\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    out = model.generate(**inputs, max_length=max_total, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "sample = processed[\"test\"][0]\n",
    "print(\"Original Dialogue:\\n\", sample[\"dialogue\"])\n",
    "print(\"Reference SOAP:\\n\", sample[\"soap\"])\n",
    "print(\"Generated SOAP:\\n\", generate_soap(sample[\"dialogue\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "70d536c7-906f-4c75-b4e3-6a50f2ea2ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32d333beed1b4bbebf17d24021a30794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating SOAP:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1/100 complete.\n",
      "Sample 2/100 complete.\n",
      "Sample 3/100 complete.\n",
      "Sample 4/100 complete.\n",
      "Sample 5/100 complete.\n",
      "Sample 6/100 complete.\n",
      "Sample 7/100 complete.\n",
      "Sample 8/100 complete.\n",
      "Sample 9/100 complete.\n",
      "Sample 10/100 complete.\n",
      "Sample 11/100 complete.\n",
      "Sample 12/100 complete.\n",
      "Sample 13/100 complete.\n",
      "Sample 14/100 complete.\n",
      "Sample 15/100 complete.\n",
      "Sample 16/100 complete.\n",
      "Sample 17/100 complete.\n",
      "Sample 18/100 complete.\n",
      "Sample 19/100 complete.\n",
      "Sample 20/100 complete.\n",
      "Sample 21/100 complete.\n",
      "Sample 22/100 complete.\n",
      "Sample 23/100 complete.\n",
      "Sample 24/100 complete.\n",
      "Sample 25/100 complete.\n",
      "Sample 26/100 complete.\n",
      "Sample 27/100 complete.\n",
      "Sample 28/100 complete.\n",
      "Sample 29/100 complete.\n",
      "Sample 30/100 complete.\n",
      "Sample 31/100 complete.\n",
      "Sample 32/100 complete.\n",
      "Sample 33/100 complete.\n",
      "Sample 34/100 complete.\n",
      "Sample 35/100 complete.\n",
      "Sample 36/100 complete.\n",
      "Sample 37/100 complete.\n",
      "Sample 38/100 complete.\n",
      "Sample 39/100 complete.\n",
      "Sample 40/100 complete.\n",
      "Sample 41/100 complete.\n",
      "Sample 42/100 complete.\n",
      "Sample 43/100 complete.\n",
      "Sample 44/100 complete.\n",
      "Sample 45/100 complete.\n",
      "Sample 46/100 complete.\n",
      "Sample 47/100 complete.\n",
      "Sample 48/100 complete.\n",
      "Sample 49/100 complete.\n",
      "Sample 50/100 complete.\n",
      "Sample 51/100 complete.\n",
      "Sample 52/100 complete.\n",
      "Sample 53/100 complete.\n",
      "Sample 54/100 complete.\n",
      "Sample 55/100 complete.\n",
      "Sample 56/100 complete.\n",
      "Sample 57/100 complete.\n",
      "Sample 58/100 complete.\n",
      "Sample 59/100 complete.\n",
      "Sample 60/100 complete.\n",
      "Sample 61/100 complete.\n",
      "Sample 62/100 complete.\n",
      "Sample 63/100 complete.\n",
      "Sample 64/100 complete.\n",
      "Sample 65/100 complete.\n",
      "Sample 66/100 complete.\n",
      "Sample 67/100 complete.\n",
      "Sample 68/100 complete.\n",
      "Sample 69/100 complete.\n",
      "Sample 70/100 complete.\n",
      "Sample 71/100 complete.\n",
      "Sample 72/100 complete.\n",
      "Sample 73/100 complete.\n",
      "Sample 74/100 complete.\n",
      "Sample 75/100 complete.\n",
      "Sample 76/100 complete.\n",
      "Sample 77/100 complete.\n",
      "Sample 78/100 complete.\n",
      "Sample 79/100 complete.\n",
      "Sample 80/100 complete.\n",
      "Sample 81/100 complete.\n",
      "Sample 82/100 complete.\n",
      "Sample 83/100 complete.\n",
      "Sample 84/100 complete.\n",
      "Sample 85/100 complete.\n",
      "Sample 86/100 complete.\n",
      "Sample 87/100 complete.\n",
      "Sample 88/100 complete.\n",
      "Sample 89/100 complete.\n",
      "Sample 90/100 complete.\n",
      "Sample 91/100 complete.\n",
      "Sample 92/100 complete.\n",
      "Sample 93/100 complete.\n",
      "Sample 94/100 complete.\n",
      "Sample 95/100 complete.\n",
      "Sample 96/100 complete.\n",
      "Sample 97/100 complete.\n",
      "Sample 98/100 complete.\n",
      "Sample 99/100 complete.\n",
      "Sample 100/100 complete.\n",
      "Saved prefix-tuning-results.csv with first 100 SOAP comparisons.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Generation helper using max_new_tokens to avoid input length issues\n",
    "def generate_soap(dialogue_text):\n",
    "    prompt = \"Summarize the following medical dialogue into a SOAP note:\\n\" + dialogue_text\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_prompt\n",
    "    ).to(model.device)\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_target,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "records = []\n",
    "for idx, sample in enumerate(tqdm(processed[\"test\"].select(range(100)), total=100, desc=\"Generating SOAP\")):\n",
    "    ref = sample[\"soap\"]\n",
    "    gen = generate_soap(sample[\"dialogue\"])\n",
    "    records.append({\"reference_soap\": ref, \"generated_soap\": gen})\n",
    "    print(f\"Sample {idx+1}/100 complete.\")\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "output_path = \"prefix-tuning-results.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Saved {output_path} with first 100 SOAP comparisons.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "12abc0be-0ce1-4da9-bb9d-b414b53b0d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking directory contents at: ./llama-3b-prefix-soap\n",
      "Directory exists. Contents:\n",
      "- adapter_config.json\n",
      "- special_tokens_map.json\n",
      "- tokenizer.json\n",
      "- tokenizer_config.json\n",
      "- adapter_model.safetensors\n",
      "- README.md\n",
      "Base model and tokenizer loaded successfully.\n",
      "PEFT model loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "100%|█████████████████████████████████████| 100/100 [14:23<00:00,  8.64s/it]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.float64' object has no attribute 'mid'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 105\u001b[39m\n\u001b[32m    101\u001b[39m bleu_result = bleu.compute(predictions=bleu_predictions, references=bleu_references)\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# Combine all evaluation results\u001b[39;00m\n\u001b[32m    104\u001b[39m results = {\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mROUGE-1\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mround\u001b[39m(rouge_result[\u001b[33m\"\u001b[39m\u001b[33mrouge1\u001b[39m\u001b[33m\"\u001b[39m].mid.fmeasure, \u001b[32m4\u001b[39m),\n\u001b[32m    106\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mROUGE-2\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mround\u001b[39m(rouge_result[\u001b[33m\"\u001b[39m\u001b[33mrouge2\u001b[39m\u001b[33m\"\u001b[39m].mid.fmeasure, \u001b[32m4\u001b[39m),\n\u001b[32m    107\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mROUGE-Lsum\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mround\u001b[39m(rouge_result[\u001b[33m\"\u001b[39m\u001b[33mrougeLsum\u001b[39m\u001b[33m\"\u001b[39m].mid.fmeasure, \u001b[32m4\u001b[39m),\n\u001b[32m    108\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mBERTScore-F1\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mround\u001b[39m(\u001b[38;5;28msum\u001b[39m(bertscore_result[\u001b[33m\"\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m\"\u001b[39m]) / \u001b[38;5;28mlen\u001b[39m(bertscore_result[\u001b[33m\"\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m\"\u001b[39m]), \u001b[32m4\u001b[39m),\n\u001b[32m    109\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mBLEU\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mround\u001b[39m(bleu_result[\u001b[33m\"\u001b[39m\u001b[33mbleu\u001b[39m\u001b[33m\"\u001b[39m], \u001b[32m4\u001b[39m)\n\u001b[32m    110\u001b[39m }\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# Display results as a table\u001b[39;00m\n\u001b[32m    113\u001b[39m results_df = pd.DataFrame([results])\n",
      "\u001b[31mAttributeError\u001b[39m: 'numpy.float64' object has no attribute 'mid'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os \n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"omi-health/medical-dialogue-to-soap-summary\")\n",
    "test_data = dataset[\"test\"]\n",
    "\n",
    "# Load evaluation metrics\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "# Load your fine-tuned model and tokenizer\n",
    "model_path = \"./llama-3b-prefix-soap\"\n",
    "\n",
    "# Check\n",
    "print(f\"Checking directory contents at: {model_path}\")\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Directory exists. Contents:\")\n",
    "    contents = os.listdir(model_path)\n",
    "    for item in contents:\n",
    "        print(f\"- {item}\")\n",
    "\n",
    "    # Check for PEFT adapter weights file (usually adapter_model.safetensors or adapter_model.bin)\n",
    "    peft_weights_found = False\n",
    "    for item in contents:\n",
    "        if item.startswith(\"adapter_model.\") and (item.endswith(\".safetensors\") or item.endswith(\".bin\")):\n",
    "            peft_weights_found = True\n",
    "            break\n",
    "\n",
    "    if not peft_weights_found:\n",
    "        print(\"Error: PEFT adapter weights file (adapter_model.safetensors or .bin) not found in the directory!\")\n",
    "        # Exit or raise an error if the weights are not found\n",
    "        exit()\n",
    "else:\n",
    "    print(\"Error: Directory does not exist!\")\n",
    "    exit()\n",
    "\n",
    "# Load the base model first, potentially with the same quantization used during training\n",
    "# Assuming you used 8-bit quantization based on the training code\n",
    "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "# Get the base model name from the saved PEFT config\n",
    "peft_config = PeftConfig.from_pretrained(model_path)\n",
    "base_model_name = peft_config.base_model_name_or_path\n",
    "\n",
    "# Load the base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\":0}\n",
    ")\n",
    "\n",
    "# Load the tokenizer from the saved path (PEFT saves the tokenizer too)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token # Ensure pad token is set\n",
    "\n",
    "print(\"Base model and tokenizer loaded successfully.\")\n",
    "\n",
    "# Load the PEFT adapter weights onto the base model\n",
    "model = PeftModel.from_pretrained(base_model, model_path)\n",
    "print(\"PEFT model loaded successfully.\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Function to generate SOAP note\n",
    "def generate_soap(dialogue_text):\n",
    "    prompt = \"Summarize the following medical dialogue into a SOAP note:\\n\" + dialogue_text\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs, max_new_tokens=256, do_sample=False)  # greedy, faster\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Generate predictions and collect references\n",
    "references = []\n",
    "predictions = []\n",
    "\n",
    "for sample in tqdm(test_data.select(range(100))):\n",
    "    ref = sample[\"soap\"]\n",
    "    pred = generate_soap(sample[\"dialogue\"])\n",
    "    references.append(ref)\n",
    "    predictions.append(pred)\n",
    "\n",
    "# Compute ROUGE scores\n",
    "rouge_result = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Compute BERTScore\n",
    "bertscore_result = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
    "\n",
    "# Prepare data for BLEU (requires tokenized inputs)\n",
    "bleu_references = [[ref] for ref in references]\n",
    "bleu_predictions = predictions\n",
    "\n",
    "bleu_result = bleu.compute(predictions=bleu_predictions, references=bleu_references)\n",
    "\n",
    "# Combine all evaluation results\n",
    "results = {\n",
    "    \"ROUGE-1\": round(rouge_result[\"rouge1\"].mid.fmeasure, 4),\n",
    "    \"ROUGE-2\": round(rouge_result[\"rouge2\"].mid.fmeasure, 4),\n",
    "    \"ROUGE-Lsum\": round(rouge_result[\"rougeLsum\"].mid.fmeasure, 4),\n",
    "    \"BERTScore-F1\": round(sum(bertscore_result[\"f1\"]) / len(bertscore_result[\"f1\"]), 4),\n",
    "    \"BLEU\": round(bleu_result[\"bleu\"], 4)\n",
    "}\n",
    "\n",
    "# Display results as a table\n",
    "results_df = pd.DataFrame([results])\n",
    "print(\"Evaluation Metrics:\\n\", results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4ebc9276-62bb-410b-be16-a73100a13898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "    ROUGE-1  ROUGE-2  ROUGE-Lsum  BERTScore-F1    BLEU\n",
      "0    0.484   0.2599      0.4351        0.8699  0.1704\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    \"ROUGE-1\": round(rouge_result[\"rouge1\"], 4),\n",
    "    \"ROUGE-2\": round(rouge_result[\"rouge2\"], 4),\n",
    "    \"ROUGE-Lsum\": round(rouge_result[\"rougeLsum\"], 4),\n",
    "    \"BERTScore-F1\": round(sum(bertscore_result[\"f1\"]) / len(bertscore_result[\"f1\"]), 4),\n",
    "    \"BLEU\": round(bleu_result[\"bleu\"], 4)\n",
    "}\n",
    "\n",
    "# Display results as a table\n",
    "results_df = pd.DataFrame([results])\n",
    "print(\"Evaluation Metrics:\\n\", results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
