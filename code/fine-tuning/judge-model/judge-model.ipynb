{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e35ce979-8e56-48a8-b579-93882aad9e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f2028f8-f5e8-47ce-b469-f63db5aa90ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visible: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Visible:\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2ce827f-25af-4a16-a00e-8c0c1a2fb4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: 0\n",
      "Device name: NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "print(\"Using device:\", torch.cuda.current_device())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f2b30874-9060-4529-bbbf-0ddc897a460e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Get the API key from the environment variable\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30f31700-1fcc-41aa-b911-10e5507f1017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key=api_key,\n",
    ")\n",
    "\n",
    "# Function to generate evaluation using Groq API\n",
    "def generate_evaluation(prompt):\n",
    "    try:\n",
    "        # Call Groq API with Llama 3 70B\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            model=\"llama3-70b-8192\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=200,  # Reduced from 500 for efficiency\n",
    "            temperature=0.3,\n",
    "            top_p=1.0\n",
    "        )\n",
    "        # Extract generated text\n",
    "        generated_text = chat_completion.choices[0].message.content\n",
    "        return generated_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating evaluation: {e}\")\n",
    "        return \"Score: [0]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1df8dbf-b57a-407d-87ae-17a7b0e58108",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files1 =  [\"prefix-tuning-results.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8825359-4dbd-4415-94da-766cce79a5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files2 = [\"transfer-learning-results.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c11b10c-4f9b-4116-af28-50d275d7753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file3 = [\"lora-finetuned-results-new.csv\", \"adaptive-learning-results.csv\", \"full-paramater-generated-results.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "744e3454-99d5-4cb1-9bbc-d82fb7b2bfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from groq import Groq\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5ce81aa-8568-469d-b04e-5c1a16c2f758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a dictionary to store results for each model\n",
    "all_results = {}\n",
    "\n",
    "for file_name in csv_files1:\n",
    "    file_path = f\"{file_name}\"\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not Path(file_path).is_file():\n",
    "        print(f\"File not found: {file_name}\")\n",
    "        continue\n",
    "    \n",
    "    # Load the CSV for the current model\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        df = df.head(50)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_name}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Verify required columns\n",
    "    \n",
    "    required_columns = [\"generated_soap\", \"reference_soap\"]\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        print(f\"Missing required columns in {file_name}: {required_columns}\")\n",
    "        continue\n",
    "    \n",
    "    # Prepare lists for storing evaluation results\n",
    "    scores = []\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        generated_soap = row[\"generated_soap\"]\n",
    "        reference_soap = row[\"reference_soap\"]\n",
    "        \n",
    "        # Prepare the evaluation prompt\n",
    "        prompt = f\"\"\"\n",
    "        Evaluate the following SOAP notes:\n",
    "\n",
    "        Reference SOAP:\n",
    "        {reference_soap}\n",
    "\n",
    "        Generated SOAP:\n",
    "        {generated_soap}\n",
    "\n",
    "        Rate the quality of the generated SOAP note on a scale of 0-10 based on the following criteria:\n",
    "        - Completeness: How much of the necessary information is included (0.25 weight)\n",
    "        - Correctness: Medical accuracy of the content (0.35 weight)\n",
    "        - Organization: Structure follows SOAP format (0.20 weight)\n",
    "        - Clinical Relevance: Relevance of the content to clinical practice (0.20 weight)\n",
    "\n",
    "        Provide only the score from 0 to 10 based on the weighted evaluation.\n",
    "        Score: [ ]\n",
    "        \"\"\"\n",
    "        \n",
    "        # Generate evaluation using Groq API\n",
    "        generated_content = generate_evaluation(prompt)\n",
    "\n",
    "        try:\n",
    "            # Extract score from the model's response\n",
    "            score_line = generated_content.split(\"Score:\")[1].split(\"\\n\")[0].strip()\n",
    "            score = float(score_line.replace(\"[\", \"\").replace(\"]\", \"\"))\n",
    "            scores.append(score)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to parse output at row {i+1} in {file_name}: {e}\")\n",
    "            scores.append(0.0)\n",
    "    \n",
    "    # Save the judged results for the current model\n",
    "    df[\"Judge Score\"] = scores\n",
    "\n",
    "    # Store the evaluated dataframe in the dictionary\n",
    "    all_results[file_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd1f765d-4f6b-4bac-9afa-3d0841e936c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for prefix-tuning-results.csv completed and saved as judged_prefix-tuning-results.csv!\n",
      "All evaluations completed!\n"
     ]
    }
   ],
   "source": [
    "# Save all judged results for each model\n",
    "for file_name, result_df in all_results.items():\n",
    "    output_file = f\"judged_{file_name}\"\n",
    "    result_df.to_csv(output_file, index=False)\n",
    "    print(f\"Evaluation for {file_name} completed and saved as {output_file}!\")\n",
    "\n",
    "print(\"All evaluations completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "672227b0-5356-471e-8989-cb7372559bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_3B_prefix_tuning=pd.read_csv(\"judged_prefix-tuning-results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "396c78d9-b163-4045-93ee-490602467186",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg1 = llama_3B_prefix_tuning[\"Judge Score\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2d7a832-2930-4cb2-af44-1b6d20569d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.968\n"
     ]
    }
   ],
   "source": [
    "print(avg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc717c2d-aece-49dc-973a-fd09094761d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a dictionary to store results for each model\n",
    "all_results = {}\n",
    "\n",
    "for file_name in csv_files2:\n",
    "    file_path = f\"{file_name}\"\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not Path(file_path).is_file():\n",
    "        print(f\"File not found: {file_name}\")\n",
    "        continue\n",
    "    \n",
    "    # Load the CSV for the current model\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        df = df.head(50)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_name}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Verify required columns\n",
    "    \n",
    "    required_columns = [\"Generated SOAP\", \"Reference SOAP\"]\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        print(f\"Missing required columns in {file_name}: {required_columns}\")\n",
    "        continue\n",
    "    \n",
    "    # Prepare lists for storing evaluation results\n",
    "    scores = []\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        generated_soap = row[\"Generated SOAP\"]\n",
    "        reference_soap = row[\"Reference SOAP\"]\n",
    "        \n",
    "        # Prepare the evaluation prompt\n",
    "        prompt = f\"\"\"\n",
    "        Evaluate the following SOAP notes:\n",
    "\n",
    "        Reference SOAP:\n",
    "        {reference_soap}\n",
    "\n",
    "        Generated SOAP:\n",
    "        {generated_soap}\n",
    "\n",
    "        Rate the quality of the generated SOAP note on a scale of 0-10 based on the following criteria:\n",
    "        - Completeness: How much of the necessary information is included (0.25 weight)\n",
    "        - Correctness: Medical accuracy of the content (0.35 weight)\n",
    "        - Organization: Structure follows SOAP format (0.20 weight)\n",
    "        - Clinical Relevance: Relevance of the content to clinical practice (0.20 weight)\n",
    "\n",
    "        Provide only the score from 0 to 10 based on the weighted evaluation.\n",
    "        Score: [ ]\n",
    "        \"\"\"\n",
    "        \n",
    "        # Generate evaluation using Groq API\n",
    "        generated_content = generate_evaluation(prompt)\n",
    "\n",
    "        try:\n",
    "            # Extract score from the model's response\n",
    "            score_line = generated_content.split(\"Score:\")[1].split(\"\\n\")[0].strip()\n",
    "            score = float(score_line.replace(\"[\", \"\").replace(\"]\", \"\"))\n",
    "            scores.append(score)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to parse output at row {i+1} in {file_name}: {e}\")\n",
    "            scores.append(0.0)\n",
    "    \n",
    "    # Save the judged results for the current model\n",
    "    df[\"Judge Score\"] = scores\n",
    "\n",
    "    # Store the evaluated dataframe in the dictionary\n",
    "    all_results[file_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4e41f53-c1fb-431b-bde9-51286454d162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for transfer-learning-results.csv completed and saved as judged_transfer-learning-results.csv!\n",
      "All evaluations completed!\n"
     ]
    }
   ],
   "source": [
    "# Save all judged results for each model\n",
    "for file_name, result_df in all_results.items():\n",
    "    output_file = f\"judged_{file_name}\"\n",
    "    result_df.to_csv(output_file, index=False)\n",
    "    print(f\"Evaluation for {file_name} completed and saved as {output_file}!\")\n",
    "\n",
    "print(\"All evaluations completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eaa4d7e3-71e8-49e0-89ab-8d3679e4e790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.325\n"
     ]
    }
   ],
   "source": [
    "bart_large_transfer_learning=pd.read_csv(\"judged_transfer-learning-results.csv\")\n",
    "avg2 = bart_large_transfer_learning[\"Judge Score\"].mean()\n",
    "print(avg2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
