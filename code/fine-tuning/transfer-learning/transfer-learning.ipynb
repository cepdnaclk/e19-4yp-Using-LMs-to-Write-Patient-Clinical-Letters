{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d2b6b5b-877c-4189-8fcb-fb33fd07467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "216d1a9e-1501-4b6d-a2cb-d8bde03738f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Training Split Size: 9250\n",
      "Original Validation Split Size: 500\n",
      "Original Test Split Size: 250\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"omi-health/medical-dialogue-to-soap-summary\")\n",
    "print(\"Original Training Split Size:\", len(dataset[\"train\"]))\n",
    "print(\"Original Validation Split Size:\", len(dataset[\"validation\"]))\n",
    "print(\"Original Test Split Size:\", len(dataset[\"test\"]))\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_dialogue(example):\n",
    "    dialogue = example[\"dialogue\"]\n",
    "    soap = example[\"soap\"]\n",
    "\n",
    "    # Clean text\n",
    "    dialogue = re.sub(r'[^A-Za-z0-9\\s.,:?-]', '', dialogue).lower()\n",
    "    soap = re.sub(r'[^A-Za-z0-9\\s.,:?-]', '', soap).lower()\n",
    "\n",
    "    # Add role tags\n",
    "    dialogue = dialogue.replace(\"Doctor:\", \"[Doctor]:\")\n",
    "    dialogue = dialogue.replace(\"Patient:\", \"[Patient]:\")\n",
    "\n",
    "    return {\"dialogue\": dialogue, \"soap\": soap}\n",
    "\n",
    "# Apply preprocessing\n",
    "processed_dataset = dataset.map(preprocess_dialogue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17124b80-c0a3-4d37-a9be-7913dcf15e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34211fce0a1c4f4a9dc90e848cd0e38f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/e19431/miniconda3/envs/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3951: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample from train_dataset: dict_keys(['prompt', 'messages', 'messages_nosystem', 'input_ids', 'attention_mask', 'labels'])\n",
      "Sample 'input_ids' shape: torch.Size([512])\n",
      "Sample 'labels' shape: torch.Size([256])\n",
      "Sample from eval_dataset: dict_keys(['prompt', 'messages', 'messages_nosystem', 'input_ids', 'attention_mask', 'labels'])\n",
      "Sample 'labels' shape (eval): torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "# Load BART tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "\n",
    "# Tokenization function for dialogue-to-SOAP task\n",
    "def tokenize_function(example):\n",
    "    # Tokenize dialogue (input)\n",
    "    model_inputs = tokenizer(\n",
    "        example[\"dialogue\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Tokenize SOAP (target)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            example[\"soap\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=256,  # Shorter for SOAP as it's more concise\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"].squeeze()\n",
    "    return model_inputs\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_dataset = processed_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove unnecessary columns and set format\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"dialogue\", \"soap\"])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "# Split into train and eval datasets\n",
    "train_dataset = tokenized_dataset[\"train\"]\n",
    "eval_dataset = tokenized_dataset[\"validation\"]\n",
    "\n",
    "# Debug: Verify dataset\n",
    "print(\"Sample from train_dataset:\", train_dataset[0].keys())\n",
    "print(\"Sample 'input_ids' shape:\", train_dataset[0][\"input_ids\"].shape)\n",
    "print(\"Sample 'labels' shape:\", train_dataset[0][\"labels\"].shape)\n",
    "print(\"Sample from eval_dataset:\", eval_dataset[0].keys())\n",
    "print(\"Sample 'labels' shape (eval):\", eval_dataset[0][\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe37623a-949a-4a9c-a35f-4425dc6a680d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_254520/782483146.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load BART model\n",
    "model= AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large\", use_safetensors=True)\n",
    "\n",
    "# Freeze BART's encoder for feature extraction\n",
    "for param in model.model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Optionally, freeze all but the last decoder layer\n",
    "for layer in model.model.decoder.layers[:-1]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bart-soap-finetuned\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    per_device_train_batch_size=8,  # Increased from 4 to 8\n",
    "    per_device_eval_batch_size=8,   # Increased from 4 to 8\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=1e-4,  # Increased learning rate to match larger batch size\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=\"none\",\n",
    "    gradient_accumulation_steps=2,  # Added to effectively increase batch size further\n",
    "    warmup_steps=500,  # Added warmup steps for better training stability\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a21850c-b8bd-4a21-b8a4-0799cfed78a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/e19431/miniconda3/envs/venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='870' max='870' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [870/870 36:16, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.048200</td>\n",
       "      <td>1.500591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.681600</td>\n",
       "      <td>1.319817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.539700</td>\n",
       "      <td>1.262581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/e19431/miniconda3/envs/venv/lib/python3.11/site-packages/transformers/modeling_utils.py:3685: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "/home/e19431/miniconda3/envs/venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/e19431/miniconda3/envs/venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=870, training_loss=1.799115788251504, metrics={'train_runtime': 2181.7035, 'train_samples_per_second': 12.719, 'train_steps_per_second': 0.399, 'total_flos': 3.0068576354304e+16, 'train_loss': 1.799115788251504, 'epoch': 3.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c53f8102-b252-4039-98ef-4293ac4cc70d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./bart-soap-finetuned-final/tokenizer_config.json',\n",
       " './bart-soap-finetuned-final/special_tokens_map.json',\n",
       " './bart-soap-finetuned-final/vocab.json',\n",
       " './bart-soap-finetuned-final/merges.txt',\n",
       " './bart-soap-finetuned-final/added_tokens.json',\n",
       " './bart-soap-finetuned-final/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./bart-soap-finetuned-final\")\n",
    "tokenizer.save_pretrained(\"./bart-soap-finetuned-final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20bd2e95-a8fa-4774-8c0b-1c738bc4a90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Dialogue: doctor: hello, weve received your results from the ultrasound we performed in april 2017. it seems that there is a single thyroid nodule present in your left lobe, measuring 1 cm in its largest diameter. we also performed a complete biochemical screening, including tests for tsh, autoantibodies, and calcitonin.\n",
      "patient: hmm, what did the screening results show, doctor?\n",
      "doctor: your calcitonin level was found to be slightly elevated at 40 ngml, which is above the normal range of 1-4.8 ngml. to further investigate, we performed a stimulation test with intravenous calcium.\n",
      "patient: and what did the stimulation test show?\n",
      "doctor: after the stimulation, your calcitonin levels peaked at 1420 ngml, which indicates that surgical treatment is necessary. as a result, you underwent a total thyroidectomy and central neck dissection on the side of the tumor.\n",
      "patient: yes, i remember that. how was my recovery after the surgery?\n",
      "doctor: your postoperative course was uneventful, with only a slight hypocalcemia recorded in the first day. however, it completely recovered 48 hours after the surgery, and you were discharged.\n",
      "patient: thats good to hear. what did the tests on the removed tissue show?\n",
      "doctor: immunohistochemistry performed on the thyroid nodule revealed the presence of a medullary thyroid cancer mtc measuring 1 cm. it is composed of cells with an eosinophilic cytoplasm and has a predominantly expansive growth pattern. a histological examination showed that the tumor cells were positive for calcitonin, cromogranin a, synaptofisin, and ttf-1, but negative for the presence of amyloid.\n",
      "patient: what about the other surrounding tissue?\n",
      "doctor: focal foci of c-cells hyperplasia were spread throughout the entire gland. however, none of the lymph nodes in the central compartment showed any signs of metastases.\n",
      "patient: thats a relief. what other tests were done on the tissue?\n",
      "doctor: to perform a more detailed analysis, we treated formalin-fixed paraffin-embedded sections with antigen retrieval using citrate buffer at high ph. they were then immunolabeled with a rabbit monoclonal anti-calcitonin antibody and incubated with appropriate fluorescent secondary antibodies.\n",
      "patient: so, what does this all mean for my condition?\n",
      "doctor: overall, these results confirm the presence of medullary thyroid cancer. fortunately, none of the lymph nodes showed signs of metastases, which is a positive sign. its important to continue with regular follow-up appointments and monitoring to ensure proper management of your condition.\n",
      "\n",
      "\n",
      "Generated SOAP: s: the patient, a male with a history of medullary thyroid cancer mtc diagnosed in april 2017, presented with a chief complaint of hypocalcemia following a total thyroidectomy and central neck dissection.\n",
      "o: physical examination revealed a single thyroid nodule in the left lobe, measuring 1 cm in its largest diameter. laboratory tests showed elevated calcitonin levels at 40 ngml normal range: 1-4.8 ngml. a histological examination showed no signs of metastases. histological tests showed no evidence of lymph nodes in the central compartment.\n",
      "a: the primary diagnosis is medulla thyroid carcinoma mtc with a 1 cm nodule. differential diagnoses could include other forms of thyroid cancer, but these are less likely given the nature of the tumor.\n",
      "p: the management plan includes surgical intervention to manage the condition.\n",
      "\n",
      "\n",
      "Reference SOAP: s: the patient reported undergoing an ultrasound in april 2017, which identified a thyroid nodule in the left lobe. the patient underwent biochemical screening, including tsh, autoantibodies, and calcitonin levels, which showed elevated calcitonin. a stimulation test with intravenous calcium indicated significantly elevated calcitonin levels, leading to a total thyroidectomy and central neck dissection. post-surgery, the patient experienced transient hypocalcemia but recovered within 48 hours. the patient inquired about the recovery and results of the tissue tests post-surgery.\n",
      "o: ultrasound revealed a 1 cm thyroid nodule in the left lobe. initial calcitonin level was 40 ngml normal range 1-4.8 ngml. post-stimulation, calcitonin levels peaked at 1420 ngml. postoperative recovery included slight hypocalcemia, resolving in 48 hours. immunohistochemistry of the thyroid nodule confirmed medullary thyroid cancer mtc, 1 cm in size, with cells positive for calcitonin, cromogranin a, synaptofisin, and ttf-1, and negative for amyloid. no lymph node metastases were found. additional tissue analysis involved antigen retrieval and immunolabeling confirming mtc.\n",
      "a: the primary diagnosis is medullary thyroid cancer, confirmed by histological and immunohistochemical analysis of the thyroid nodule and absence of metastasis in lymph nodes. the patients elevated calcitonin levels and the results of the stimulation test supported the need for surgical intervention, which was successfully performed without complications.\n",
      "p: the patient will continue with regular follow-up appointments to monitor for any recurrence or progression of medullary thyroid cancer. monitoring will include periodic calcitonin levels and ultrasound imaging of the neck. the patient will be educated on the signs of recurrence and the importance of adherence to follow-up schedules. further genetic counseling may be considered to evaluate for familial mtc.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./bart-soap-finetuned-final\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./bart-soap-finetuned-final\")\n",
    "\n",
    "# Inference function\n",
    "def generate_soap(dialogue, model, tokenizer, device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"):\n",
    "    model.to(device)\n",
    "    inputs = tokenizer(\n",
    "        dialogue,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    ).to(device)\n",
    "    \n",
    "    # Generate SOAP report\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=256,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test inference on a sample\n",
    "sample_dialogue = processed_dataset[\"test\"][2][\"dialogue\"]\n",
    "sample_soap = processed_dataset[\"test\"][2][\"soap\"]\n",
    "print(\"\\nSample Dialogue:\", sample_dialogue)\n",
    "print(\"\\n\\nGenerated SOAP:\", generate_soap(sample_dialogue, model, tokenizer))\n",
    "print(\"\\n\\nReference SOAP:\", sample_soap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4694ce66-731e-4828-b5f4-3d4fbcc1e447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to 'transfer-learning-results.csv' ✅\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./bart-soap-finetuned-final\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./bart-soap-finetuned-final\")\n",
    "\n",
    "# Inference function\n",
    "def generate_soap(dialogue, model, tokenizer, device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    inputs = tokenizer(\n",
    "        dialogue,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_length=256,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Generate and collect results for the first 100 samples\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "results = []\n",
    "\n",
    "for idx in range(100):\n",
    "    sample_dialogue = processed_dataset[\"test\"][idx][\"dialogue\"]\n",
    "    reference_soap = processed_dataset[\"test\"][idx][\"soap\"]\n",
    "    \n",
    "    generated_soap = generate_soap(sample_dialogue, model, tokenizer, device=device)\n",
    "    \n",
    "    results.append({\n",
    "        \"Dialogue\": sample_dialogue,\n",
    "        \"Reference SOAP\": reference_soap,\n",
    "        \"Generated SOAP\": generated_soap\n",
    "    })\n",
    "\n",
    "# Save the results to a CSV file\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"transfer-learning-results.csv\", index=False)\n",
    "\n",
    "print(\"Results saved to 'transfer-learning-results.csv' ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3beac09-d8fb-4e3c-8185-5c74edf2d841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ROUGE Results:\n",
      "{'rouge1': np.float64(0.532743784942171), 'rouge2': np.float64(0.305151374383853), 'rougeL': np.float64(0.3494364611273041), 'rougeLsum': np.float64(0.44977932903076123)}\n",
      "\n",
      "BLEU Results:\n",
      "{'bleu': 0.20922846183832733, 'precisions': [0.7170381987229753, 0.43804213135068154, 0.29455081001472755, 0.20753104705480233], 'brevity_penalty': 0.5620764554238586, 'length_ratio': 0.6344705046197584, 'translation_length': 17854, 'reference_length': 28140}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BERTScore Results:\n",
      "{'precision': 0.9102424734830856, 'recall': 0.8778711241483689, 'f1': 0.8936530894041061}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import pandas as pd\n",
    "\n",
    "# Load your saved CSV\n",
    "df = pd.read_csv(\"transfer-learning-results.csv\")\n",
    "\n",
    "# Extract generated and reference lists\n",
    "generated_soap_list = df[\"Generated SOAP\"].tolist()\n",
    "reference_soap_list = df[\"Reference SOAP\"].tolist()\n",
    "\n",
    "# Load evaluation metrics\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "# Compute ROUGE\n",
    "rouge_result = rouge.compute(predictions=generated_soap_list, references=reference_soap_list)\n",
    "print(\"\\nROUGE Results:\")\n",
    "print(rouge_result)\n",
    "\n",
    "# Compute BLEU\n",
    "# BLEU expects references as a list of lists\n",
    "bleu_result = bleu.compute(predictions=generated_soap_list, references=[[ref] for ref in reference_soap_list])\n",
    "print(\"\\nBLEU Results:\")\n",
    "print(bleu_result)\n",
    "\n",
    "# Compute BERTScore\n",
    "bertscore_result = bertscore.compute(predictions=generated_soap_list, references=reference_soap_list, lang=\"en\")\n",
    "print(\"\\nBERTScore Results:\")\n",
    "print({\n",
    "    \"precision\": sum(bertscore_result[\"precision\"]) / len(bertscore_result[\"precision\"]),\n",
    "    \"recall\": sum(bertscore_result[\"recall\"]) / len(bertscore_result[\"recall\"]),\n",
    "    \"f1\": sum(bertscore_result[\"f1\"]) / len(bertscore_result[\"f1\"])\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
